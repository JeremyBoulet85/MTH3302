{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Débordement d'égouts\n",
    "## Rapport final\n",
    "#### Équipe 15: Jeremy Boulet, Duc-Thien Nguyen, Olivia You-Tuon et Steven Lam\n",
    "#### 20 décembre 2019\n",
    "\n",
    "---\n",
    "\n",
    "## Table des matières\n",
    "\n",
    "- [1. Introduction](#introduction)\n",
    "    - [1.1 Objectif](#goal)\n",
    "    - [1.2 Variables explicatives](#varExplicative)\n",
    "- [2. Méthode](#method)\n",
    "    - [2.1 Les modèles](#models)\n",
    "        - [2.1.1 Logistic Regression](#logit)\n",
    "        - [2.1.2 Random Forest Tree](#rft)\n",
    "        - [2.1.3 Naive-Bayesian](#naive)\n",
    "- [3. Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## 1. Introduction\n",
    "\n",
    "<a id=\"goal\"></a>\n",
    "### 1.1 Objectif\n",
    "L'objectif de ce projet est de prédire, à l'aide de jeux de données de la ville de Montréal, les débordements futurs de 5 ouvrages pour une partie de 2019. Le sous-objectif est d'identifier les variables explicatives qui permettront de faire les prédictions les plus précises. Avec les données, il faut aussi gérer les valeurs manquantes et les valeurs abérantes.\n",
    "\n",
    "<a id=\"varExplicative\"></a>\n",
    "### 1.2 Variables explicatives\n",
    "En analysant les données fournies par l'énnoncé, nous avons déterminer que nos variables explicatives sont: \n",
    "\n",
    "- __Ajouter__: les variables explicatives\n",
    "- __Précipitation__ : Precipitation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"method\"></a>\n",
    "## 2. Méthode\n",
    "\n",
    "<a id=\"models\"></a>\n",
    "### 2.1 Les modèles\n",
    "Pour ce projet, nous avons opter d'utiliser trois modèles différents: Logistic Regression, Random Forest Tree et Naive-Bayesian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logit\"></a>\n",
    "#### 2.1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les codes de logit  et justifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rft\"></a>\n",
    "#### 2.1.2 Ranfom Forest Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les codes de RFT et justifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"naive\"></a>\n",
    "#### 2.1.3 Naive-Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les codes de RFT et justifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En deuxième lieu, nous avons également utiliser un modèle suivant la méthode de classification bayésienne naïve. Bien que ce n'était pas le cas, nous devions donc faire la supposition que les variables explicatives étaient indépendantes entre elles.\n",
    "\n",
    "Nous avons décidé d'utiliser la librairie ScikitLearn pour générer le modèle. \n",
    "\n",
    "Pour notre premier essai, nous voulions uniquement tester la librairie. Alors, nous avons supposé, sans vérifier, que les variables explicatives pouvaient être modèlisées par la loi normale afin d'utiliser le modèle GaussianNB de ScikitLearn.\n",
    "\n",
    "Pour notre deuxième essai, nous avons vérifié que les variables explicatives ne suivaient pas vraiment une distribution normale. Nous avons donc décidé d'utiliser le modèle MultinomialNB. Il était nécessaire de formatter nos variables pour qu'elles puissent être modèlisées par la loi multinomiale. Pour ce faire, nous avons séparé les variables explicatives en catégories:\n",
    "\n",
    "\n",
    "| Catégorie | Somme de précipitation journalière | \n",
    "| --- | --- |\n",
    "| sum1 | 0 |\n",
    "| sum2 | ]0, 50\\[ |\n",
    "| sum3 | \\[50, 100[ |\n",
    "| sum4 | \\[100, 150[ |\n",
    "| sum5 | \\[150, 200[ |\n",
    "| sum6 | \\[200, 250[ |\n",
    "| sum7 | \\[250, 300[ |\n",
    "| sum8 | \\[300, 350[ |\n",
    "| sum9 | \\[350, 400[ |\n",
    "| sum10 | \\[400, 450[ |\n",
    "| sum11 | \\[450, 500[ |\n",
    "| sum12 | >= 500 |\n",
    "\n",
    "| Catégorie | Max horaire de précipitation journalier |\n",
    "| --- | --- |\n",
    "| max1 | 0 |\n",
    "| max2 | ]0, 25\\[ |\n",
    "| max3 | \\[25, 50[ |\n",
    "| max4 | \\[50, 75[ |\n",
    "| max5 | \\[75, 100[ |\n",
    "| max6 | \\[100, 125[ |\n",
    "| max7 | \\[125, 150[ |\n",
    "| max8 | \\[150, 175[ |\n",
    "| max9 | \\[175, 200[ |\n",
    "| max10 | >= 200 |\n",
    "\n",
    "| Catégorie | Trop Plein Z |\n",
    "| --- | --- |\n",
    "| z1 | 0 |\n",
    "| z2 | ]0, 15\\[ |\n",
    "| z3 | \\[15, 20[ |\n",
    "| z4 | >= 20 |\n",
    "\n",
    "Ensuite, nous subtituons les valeurs de chaque variable par sa catégorie correspondante. Nous comptons les fréquences de chaque catégorie par rapport à la date en utilisant un CountVectorizer de la libraire ScikitLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
